{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lynma\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lynma\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch # In anaconda prompt: pip install -U torch\n",
    "from transformers import AutoTokenizer, AutoModel # In anaconda prompt: pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "DATA_URL = 'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    "N_ROWS = 1000 # number of rows to read from input file.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Load data\n",
    "- Use `pd.read_csv` to load data from the `DATA_URL` specified above<br>\n",
    "(**NOTE**: Specify parameters `delimiter='\\t'` and `header=None`, since this data file is a `.tsv` file without header columns)\n",
    "\n",
    "- Rename the columns (`df.rename`). The text column should renamed to 'text' and label column should be called 'label'\n",
    "- Print the top three rows\n",
    "- Print the value counts of the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  a stirring , funny and finally transporting re...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo n't sit still f...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data and renaming the columns: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html\n",
    "df = pd.read_csv(DATA_URL, nrows=N_ROWS, delimiter='\\t', header=None)\n",
    "df = df.rename(columns={0: \"text\", 1: \"label\"})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    521\n",
      "0    479\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count values in name column. https://www.geeksforgeeks.org/how-to-count-occurrences-of-specific-value-in-pandas-column/\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Load model weights\n",
    "The HuggingFace (transformers) ecosystem allows us to build down model weights for pre-trained transformer neural networks. By passing in the name (MODEL_NAME) of the model we want to use, we can load the weights into a model object automatically. \n",
    "The same thing goes for tokenizers. Most models have different tokenization schemas, which means that we want to load the tokenizer schema that works for the particular model we specified.\n",
    "\n",
    "- Run the cell below to load the model weights and tokenization schema into the Model and Tokenizer objects.\n",
    "\n",
    "- Print the `model` object. Notice how it is made up of layers just like the neural networks we trained for image classification in a prior lab, albeit a bit more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b708b6013ff2453ab49a538f18f66bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lynma\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lynma\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f652a6f74941b1b0b9d65a8545a439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63971c9625d0418fbad5382ba796d7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3b2377e09e43d8b299cc4e50260276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the model\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Tokenize the data\n",
    "With our tokenizer loaded, we can now preprocess our data into the format that the BERT model expects.\n",
    "\n",
    "- Tokenize the data by using the `__call__` method of the tokenizer object<br><br>\n",
    "    - e.g., `tokenized=tokenizer(df[\"text\"].to_list())`\n",
    "    - Pass the following arguments, along with the texts\n",
    "        - `add_special_tokens=True` (Adds the [CLS] and [SEP] tokens that the BERT model expects\")\n",
    "        - `padding='longest'` (The texts have uneven length. Padding means to insert dummy tokens to make the equal)\n",
    "        - `return_attention_mask=True` (Return the attention mask expected by the BERT model)\n",
    "        - `return_tensors='pt'` (Return the input ID tensors expected by the BERT model as PyTorch tensors)\n",
    "        - `verbose=True` (Tell us what's going on)\n",
    "<br><br>\n",
    "- Print the `tokenized` object (It will be a dictionary of `ìnput_ids` (as tensors) and `attention_mask` (as tensors))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Calculate embedding features\n",
    "With our data tokenized such that the BERT model can understand it, we are now ready to calculate the embeddings.\n",
    "\n",
    "- Use the function below to extract embeddings\n",
    "\n",
    "- Print the shape of the embeddings. Should be (N_ROWS, 758)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(model, tokenized):\n",
    "\n",
    "    \"\"\" Calculate BERT embeddings for a batch of sentences.\n",
    "    NOTE: Calculating BERT embeddings is a very expensive operation.\n",
    "    Particularly on CPU, it can take a long time to calculate embeddings for\n",
    "    a large batch of sentences (Max 10-20 minutes for 6K sentences).\n",
    "\n",
    "    Args:\n",
    "        model (transformers BERT model): BERT model.\n",
    "        tokenized (dict): Dictionary of tokenized sentences (input_ids and attention_mask)\n",
    "\n",
    "    Returns:\n",
    "        n-d NumPy array: BERT embeddings for the sentences in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Getting model encodings...\")\n",
    "    # The following is a context-manager that disables gradient calculation.\n",
    "    # Disabling gradient calculation is useful for inference, when you are \n",
    "    # sure that you will not call Tensor.backward(). It will reduce memory \n",
    "    # consumption for computations that would otherwise have requires_grad=True.\n",
    "    # TLDR: calculating gradients is expensive. We don't need them for inference.\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(**tokenized)\n",
    "\n",
    "    # last_hidden_states[0] is the last hidden state of the first token of the\n",
    "    # sequence (classification token) further processed by a Linear layer and \n",
    "    # a Tanh activation function. The Linear layer weights are trained from the\n",
    "    #  next sentence prediction (classification) objective during pretraining.\n",
    "    # last_hidden_states[0].shape = (batch_size, hidden_size)\n",
    "    print(\"Returning embeddings...\")\n",
    "    return last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "embeddings = get_bert_embeddings(model, tokenized)\n",
    "embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Train a model\n",
    "Now that we have our embeddings, it is time to use them in a machine learning model. (You can use Keras too, if you feel adventurous.)\n",
    "\n",
    "- Split the embeddings and the labels into (X_train, y_train) and (X_test, y_test) using sklearn's `train_test_split` function\n",
    "\n",
    "- Fit a Logistic Regression model on (X_train, y_train)\n",
    "\n",
    "- Evaluate the results on both the training data and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus exercises\n",
    "As discussed, the remaining tasks are bonus tasks. You are not expected to complete these before you hand in. It is just for you own understanding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 - Compare with CountVectorizer\n",
    "\n",
    "- Split the original data using train_test_split\n",
    "- Fit a CountVectorizer to (X_train)\n",
    "- Transform X_train and X_test\n",
    "- Fit a logistic regression model to the countvectorized X_train and y_train\n",
    "- Evaluate the results on both (X_train, y_train) and (X_test, y_test)\n",
    "- Compare with the results of using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
