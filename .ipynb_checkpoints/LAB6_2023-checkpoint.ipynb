{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CINTO4003U.LA_F23 (Artificial Intelligence and Machine Learning)](https://cbscanvas.instructure.com/courses/32010) - Copenhagen Business School<br>\n",
    "***\n",
    "<br>\n",
    "\n",
    "# Lab 6: Expert systems\n",
    "\n",
    "**Expert systems** are computer programs that mimic the decision-making abilities of a human expert in a particular domain or field. They use a set of rules and knowledge-based reasoning to provide specific advice or solutions to problems. On the other hand, machine learning and AI are approaches to creating intelligent systems that can learn inductively from data without being explicitly programmed.\n",
    "\n",
    "Expert systems are often overlooked due to the current hype surrounding AI. Clients and businesses are eager to implement the latest AI solutions, even when simpler solutions may suffice. Although developing an AI system may be more complex and time-consuming than developing an expert system, the latter can still provide valuable solutions for specific problems, especially in domains with well-established rules and knowledge. In some cases, an expert system may be a more efficient and accurate solution than an AI system, making it important not to overlook their potential.\n",
    "\n",
    "* Expert systems can be more suitable when the problem domain is well-understood and there is a body of established rules and knowledge available. In such cases, it can be a simple matter of encoding the rules and knowledge into the expert system. Often, this approach can provide more accurate and consistent results than machine learning.\n",
    "\n",
    "* Likewise, when explanability is essential, expert systems can be a better choice than machine learning and AI systems. They make their decisions based on explicit rules and knowledge, and the reasoning behind their decisions can be explained to the end-users. This transparency can be crucial in some domains, such as medicine and law, where decisions need to be justified and explained.\n",
    "\n",
    "### Using expert systems with AI\n",
    "It is often a good idea to start off with an expert system based on rules made by subject-matter experts because it allows for a more controlled approach to problem-solving. Subject-matter experts understand the nuances and intricacies of the domain, and they can provide the necessary rules and knowledge to make the expert system more effective. Additionally, an expert system can serve as a baseline for comparison when transitioning to a machine learning or AI system. The rules and knowledge that form the basis of an expert system can be used to train a machine learning model, allowing for a more accurate and robust system. In this way, expert systems can serve as a stepping stone towards more advanced AI solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "In this lab we will introduce [`human-learn`](https://koaning.github.io/human-learn/index.html), a brilliant library for creating expert systems that works in scikit-learn pipelines. Specifically, we will make use of the `FunctionClassifier` class, which works the same way as the scikit-learn classifiers that you are used to. The difference is that you have to pass expert system functions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install human-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, make_scorer, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from hulearn.datasets import load_titanic\n",
    "from hulearn.classification import FunctionClassifier\n",
    "from hulearn.experimental.interactive import parallel_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data\n",
    "Here we make a few tweaks, so we can jump straight to modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = load_titanic(as_frame=True)\n",
    "\n",
    "titanic_df['sex'] = titanic_df['sex'].map({'female': 0, 'male': 1})\n",
    "titanic_df['family_size'] = titanic_df['sibsp'] + titanic_df['parch'] + 1\n",
    "titanic_df['alone'] = np.where(titanic_df['family_size'] == 1, 1, 0)\n",
    "\n",
    "# For the X we drop target variable and name. For y we set the target variable.\n",
    "X, y = titanic_df.drop(columns=['survived', 'name']), titanic_df['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the data to get an overview\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dummy classifier to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = DummyClassifier(strategy='most_frequent').fit(X_train, y_train).predict(X_train)\n",
    "print(f\"Our baseline accuracy is: {accuracy_score(y_train, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our first FunctionClassifier\n",
    "Let's introduce the FunctionClassifier. As mentioned above, it works in the same way as a regular scikit-learn classifier, except for two big differences\n",
    "- The `fit` method does nothing, because the is no learning algorithm to fit. The rules are static.  (It still needs to be called though)\n",
    "- A function containing one or more rules must be passed to the classifier \n",
    "\n",
    "let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fare_based(dataf, threshold=15):\n",
    "\n",
    "    \"\"\" \n",
    "    This expert system function returns 1 ('survived') \n",
    "    if the fare price is greater than the threshold \n",
    "    or 0 ('didn't survive') otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (dataf['fare'] > threshold).astype(int)\n",
    "\n",
    "xp_clf = FunctionClassifier(fare_based, threshold=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (\n",
    "    xp_clf\n",
    "    .fit(X_train, y_train) # this step does nothing but is required\n",
    "    .predict(X_train) # predict using our expert system\n",
    ")\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, this simple rule does quite a bit better than our baseline! <br>\n",
    "**But** how do we know which threshold to pass to the function? We can gridsearch it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch expert rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = FunctionClassifier(fare_based, threshold=10)\n",
    "grid = GridSearchCV(\n",
    "    mod, \n",
    "    cv=2, \n",
    "    param_grid={'threshold': np.linspace(0, 100, 30)}, #https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\n",
    "    scoring={'accuracy': make_scorer(accuracy_score), \n",
    "              'precision': make_scorer(precision_score),\n",
    "              'recall': make_scorer(recall_score),\n",
    "              },\n",
    "    refit='accuracy'\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: {:.3f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(grid.score(X_train, y_train)))\n",
    "print(\"Best params: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_df = (\n",
    "    pd.DataFrame(grid.cv_results_)\n",
    "    .set_index('param_threshold')\n",
    "    [['mean_test_accuracy', 'mean_test_precision', 'mean_test_recall']]\n",
    ")\n",
    "\n",
    "score_df.plot(\n",
    "    figsize=(12, 5), \n",
    "    title=\"scores vs. fare-threshold\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding rules\n",
    "\n",
    "We can add as many rules as we want to, to our neat little expert system. \n",
    "Maybe we could say\n",
    "- IF 'fare' > t\n",
    "- OR 'sex' == s\n",
    "\n",
    "THEN predict 1 else 0\n",
    "\n",
    "We might make the second rule because we heuristically assume that women have a higher chance of surviving shipwrecks.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_factory(\n",
    "        dataf,\n",
    "        r1_fare_threshold = 15, \n",
    "        r2_gender_category = 0,\n",
    "    ):\n",
    "\n",
    "    # rule 1: if the fare is greater than the threshold, then survived\n",
    "    rule1 = (dataf['fare'] > r1_fare_threshold)\n",
    "\n",
    "    # rule 2: if the gender is equal to the gender category, then survived\n",
    "    rule2 = (dataf['sex'] == r2_gender_category)\n",
    "    \n",
    "    return (rule1 | rule2).astype(int) # 1 if either rule is true, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_clf = FunctionClassifier(\n",
    "    rule_factory,\n",
    "    r1_fare_threshold=15,\n",
    "    r2_gender_category=0,\n",
    ")\n",
    "\n",
    "y_pred = (\n",
    "    xp_clf\n",
    "    .fit(X_train, y_train) # this step does nothing but is required\n",
    "    .predict(X_train) # predict using our expert system\n",
    ")\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch with more rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = FunctionClassifier(rule_factory, r1_fare_threshold=10, r2_gender_category=0)\n",
    "grid = GridSearchCV(\n",
    "    mod, \n",
    "    cv=2, \n",
    "    param_grid={\n",
    "        'r1_fare_threshold': np.linspace(0, 100, 30),\n",
    "        'r2_gender_category': [0, 1],\n",
    "    },\n",
    "    scoring={\n",
    "        'accuracy': make_scorer(accuracy_score), \n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "    },\n",
    "    refit='accuracy',\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: {:.3f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(grid.score(X_train, y_train)))\n",
    "print(\"Best params: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = (\n",
    "    pd.DataFrame(grid.cv_results_)\n",
    "    [[\n",
    "        'param_r1_fare_threshold', \n",
    "        'param_r2_gender_category', \n",
    "        \n",
    "        'mean_test_accuracy', 'mean_test_precision', 'mean_test_recall']]\n",
    "    .sort_values('mean_test_accuracy', ascending=False)\n",
    "    .head(5)   \n",
    ")\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find rules using hi plot\n",
    "The `parallel_coordinates` from the `hulearn` library gives us HTML rendering of a *hi plot*. Each column is a feature in our dataset. Each line is a row in our dataset (a training example). The color indicates which class (survived or not) that the given example belongs to.\n",
    "By clicking and dragging on the columns, we can filter the examples. This can be used as inspiration to decide on new expert system rules to come up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.merge(y_train, left_index=True, right_index=True)\n",
    "\n",
    "parallel_coordinates(train, label=\"survived\", height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "- Come up with **at least** 1 new rule (Mabe something with age? Perhaps using the hi plot above?)\n",
    "- Add the new rule to the `rule_factory` function and use it to create a new `FunctionClassifier`. \n",
    "- Evaluate the results on (X_train, y_train) using a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a second rule factory with a third rule\n",
    "def rule_factory2(\n",
    "        dataf,\n",
    "        r1_fare_threshold = 15, \n",
    "        r2_gender_category = 0,\n",
    "        r3_parch_category = 2\n",
    "    ):\n",
    "\n",
    "    # rule 1: if the fare is greater than the threshold, then survived\n",
    "    rule1 = (dataf['fare'] > r1_fare_threshold)\n",
    "\n",
    "    # rule 2: if the gender is equal to the gender category, then survived\n",
    "    rule2 = (dataf['sex'] == r2_gender_category)\n",
    "\n",
    "    # rule 3\n",
    "    rule3 = (dataf['parch'] > r3_parch_category)\n",
    "    \n",
    "    return (rule1 | rule2 | rule3).astype(int) # 1 if either rule is true, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the new rule factory in the function classifier\n",
    "xp_clf2 = FunctionClassifier(\n",
    "    rule_factory2, \n",
    "    # By not defining the values, the same values as defined in the function are used. If change were needed, one could apply them here.\n",
    "    r1_fare_threshold = 15, \n",
    "    r2_gender_category = 0,\n",
    "    r3_parch_category = 2\n",
    ")\n",
    "\n",
    "# Printing the result of applying the new rule.\n",
    "y_pred = (\n",
    "    xp_clf2\n",
    "    .fit(X_train, y_train) # this step does nothing but is required\n",
    "    .predict(X_train) # predict using our expert system\n",
    ")\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "- Gridsearch your new FunctionClassifier (as above)\n",
    "- Report on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining params and scoring to be used in Grid Search\n",
    "params = {\n",
    "        'r1_fare_threshold': np.linspace(0, 100, 30),\n",
    "        'r2_gender_category': [0, 1],\n",
    "        'r3_parch_category':np.linspace(0,6,1)\n",
    "        }\n",
    "\n",
    "scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score), \n",
    "        'precision': make_scorer(precision_score),\n",
    "        'recall': make_scorer(recall_score)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the actual GridSearchCV here\n",
    "grid2 = GridSearchCV(\n",
    "    cv = 2,\n",
    "    estimator = xp_clf2,\n",
    "    param_grid = params,\n",
    "    scoring = scoring,\n",
    "    verbose = 3,\n",
    ")\n",
    "\n",
    "grid2.fit(X_train, y_train)\n",
    "print(\"Best parameters\", grid2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "* Evaluate the best gridsearch model on (X_test, y_test) (using `grid.predict()`)\n",
    "* Report on the results, and consider the precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "* Build a logistic regression classifier\n",
    "* Train it on (X_train, y_train)\n",
    "* Evaluate it on (X_test, y_test)\n",
    "* Compare with the gridsearched FunctionClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d351b74f69e41aba1f142ec59357f1d520e25744e5bcf1f914b368d17a6c3fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
